{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wheteher GPU is being used and select GPU\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta X, Delta Z, Theta, 5 range sensors\n",
    "N_STATES = 8\n",
    "# Move forward, turn right, turn left\n",
    "N_ACTIONS = 3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(N_STATES, 50)\n",
    "        self.layer1.weight.data.normal_(0, 0.2)\n",
    "        self.layer2 = nn.Linear(50, 25)\n",
    "        self.layer2.weight.data.normal_(0, 0.2)\n",
    "        self.layer3 = nn.Linear(25, 20)\n",
    "        self.layer3.weight.data.normal_(0, 0.2)\n",
    "        self.out = nn.Linear(20, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.out(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation NN\n",
    "eval_net = Net().to(device)\n",
    "print(eval_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer and MSE as loss function\n",
    "optimizer = torch.optim.Adam(eval_net.parameters(), lr=0.001)\n",
    "loss_func = nn.MSELoss()\n",
    "print(optimizer)\n",
    "print(loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Target NN\n",
    "target_net = Net().to(device)\n",
    "print(target_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay memory definition (States, actions, reward, future states)\n",
    "MEMORY_CAPACITY = 10000\n",
    "MEMORY = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))  \n",
    "print(MEMORY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Unity VE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with Unity. Sometimes, Ubuntu doesn't close the port immediately and throws exception\n",
    "n = True\n",
    "counter = 0\n",
    "while(n):\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=None, base_port=5004)\n",
    "        env.reset()\n",
    "        group_name = env.get_agent_groups()[0] \n",
    "        print('Connected...')\n",
    "        n = False\n",
    "    except:\n",
    "        counter += 1\n",
    "        time.sleep(1)\n",
    "        if (counter > 11):\n",
    "            print('Connection failed...')\n",
    "            n = False\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the agent groups in the simulation environment and use first agent group and specifications\n",
    "group_name = env.get_agent_groups()[0]\n",
    "group_spec = env.get_agent_group_spec(group_name)\n",
    "print(group_name)\n",
    "print(group_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN interacts with Unity VE to learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interact BATCH_SIZE to fill the memory with BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning. Used to restart training\n",
    "eval_net.load_state_dict(torch.load('dqn_a.dat'))\n",
    "print(eval_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up mini-batch size\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the VE is communicating with Python correctly\n",
    "step_result = env.get_step_result(group_name)\n",
    "s = torch.FloatTensor(step_result.obs[0]).to(device)\n",
    "sAgentNo = step_result.n_agents()\n",
    "action = eval_net(s)\n",
    "MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "print('Delta X:', s[0][0], '/ Delta Z:', s[0][1], '/ Facing angle:', s[0][2])\n",
    "print('Agent:', sAgentNo)\n",
    "print('NN output:', action)\n",
    "print('Action to take:', ActionIdxArray)\n",
    "print('Current reward:', step_result.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill replay memory\n",
    "MemoryIdx = 0\n",
    "step_result = env.get_step_result(group_name)\n",
    "s = torch.FloatTensor(step_result.obs[0]).to(device)\n",
    "sAgentNo = step_result.n_agents()\n",
    "while (MemoryIdx < 10000):\n",
    "    action = eval_net(s)\n",
    "    MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "    ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "    env.set_actions(group_name, ActionIdxArray)\n",
    "    env.step()\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    s_ = step_result.obs[0]\n",
    "    s_ = torch.FloatTensor(s_).to(device)\n",
    "    s_AgentNo = step_result.n_agents()\n",
    "    reward = step_result.reward\n",
    "    done = step_result.done\n",
    "    \n",
    "    if(sAgentNo == s_AgentNo ):\n",
    "        for agentIdx in range(sAgentNo):\n",
    "            transition = np.hstack((s[agentIdx].cpu().numpy(), \n",
    "                                    ActionIdxArray[agentIdx], reward[agentIdx], \n",
    "                                    s_[agentIdx].cpu().numpy()))\n",
    "            MEMORY[MemoryIdx, :] = transition\n",
    "            MemoryIdx += 1\n",
    "            if(MemoryIdx == MEMORY_CAPACITY):\n",
    "                break;\n",
    "    s = s_\n",
    "    sAgentNo= s_AgentNo\n",
    "print('Done...!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with Unity\n",
    "n = True\n",
    "counter = 0\n",
    "while(n):\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=None, base_port=5004)\n",
    "        env.reset()\n",
    "        group_name = env.get_agent_groups()[0] \n",
    "        print('Connected...')\n",
    "        n = False\n",
    "    except:\n",
    "        counter += 1\n",
    "        time.sleep(1)\n",
    "        if (counter > 11):\n",
    "            print('Connection failed...')\n",
    "            n = False\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up NN and training hyperparameters\n",
    "GAMMA = 0.95 # Penalize for each future step\n",
    "UPDATE_RATE = 10  # How many steps before update optimization\n",
    "EPSILON=0.3 # Chance of select randomly the maxima reward from the NN\n",
    "EPSILON_Min=0 \n",
    "EPSILON_DECAY=0.90 # The chance of being selected gradually decreases\n",
    "TAU = 2e-3 # Update evaluation NN (soft updating)\n",
    "episodes = 300\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LossLst = []    # Mean loss of each episode\n",
    "RewardLst = []  # Accumulated reward per episode\n",
    "StepLst = []   # Total steps in episode\n",
    "print(\"Episode:\", end = \"\")\n",
    "for episodeIdx in range(episodes):\n",
    "    if (episodeIdx % 10 == 0):\n",
    "        print(episodeIdx, end = \", \")\n",
    "    \n",
    "    # Decay of selection probability\n",
    "    EPSILON = max(EPSILON_Min, EPSILON*EPSILON_DECAY) \n",
    "    \n",
    "    # Get initial state of agents and convert to Tensor to use GPU\n",
    "    env.reset()\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    s = torch.FloatTensor(step_result.obs[0]).to(device)\n",
    "\n",
    "    EpochDone = False\n",
    "    rewardSum = 0\n",
    "    lossSum = 0\n",
    "    steps = 1\n",
    "    while (not EpochDone):\n",
    "        # Send initial state to NN to calculate probability of actions\n",
    "        action = eval_net(s)\n",
    "        # Get index of maximum Q-value for each agent\n",
    "        MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "        ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "        # Probability of performing random action\n",
    "        if (np.random.uniform() < EPSILON):   # Epsilon greedy rule\n",
    "            for i in range(ActionIdxArray.shape[0]):\n",
    "                ActionIdxArray[i]= np.random.randint(N_ACTIONS) \n",
    "        env.set_actions(group_name, ActionIdxArray)\n",
    "        env.step()\n",
    "        step_result = env.get_step_result(group_name)\n",
    "        s_ = step_result.obs[0]\n",
    "        s_ = torch.FloatTensor(s_).to(device)\n",
    "        s_AgentNo = step_result.n_agents()\n",
    "        reward = step_result.reward\n",
    "        rewardSum = rewardSum + np.average(reward)\n",
    "        done = step_result.done\n",
    "        # Check whether any agent is done\n",
    "        for elt in done:\n",
    "            if elt:  \n",
    "                EpochDone = True\n",
    "\n",
    "        # Check the integrity of the number of agents\n",
    "        if(sAgentNo == s_AgentNo ):\n",
    "            for agentIdx in range(sAgentNo):\n",
    "                transition = np.hstack((s[agentIdx].cpu().numpy(), ActionIdxArray[agentIdx], \n",
    "                                        reward[agentIdx], s_[agentIdx].cpu().numpy()))\n",
    "                MEMORY[MemoryIdx%MEMORY_CAPACITY,:] = transition\n",
    "                MemoryIdx += 1\n",
    "        s = s_\n",
    "        sAgentNo= s_AgentNo\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        if(steps % UPDATE_RATE == 0):\n",
    "            # Sample a mini-batch from the replay memory.\n",
    "            # The sample must be random to let the target NN converge properly; otherwise,\n",
    "            # the target NN could converge only for the last steps and forget everything else.\n",
    "            sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "            b_memory = MEMORY[sample_index, :]\n",
    "            b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "            b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "            b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "            b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "            q_eval = eval_net(b_s).gather(1, b_a)\n",
    "            q_next = target_net(b_s_).detach()\n",
    "            q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "            loss = loss_func(q_eval, q_target)\n",
    "            lossSum = lossSum + float(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Perform soft update in target NN\n",
    "            for target_param, local_param in zip(target_net.parameters(), eval_net.parameters()):\n",
    "                target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)\n",
    "    \n",
    "        steps += 1\n",
    "        \n",
    "    # Record data\n",
    "    LossLst.append(lossSum/steps)\n",
    "    RewardLst.append(rewardSum/steps)\n",
    "    StepLst.append(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the agent has learnt, close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards and steps\n",
    "fig = plt.figure(figsize=(18,18)) \n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=20)\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(RewardLst)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(StepLst)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Steps\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(LossLst)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NN parameters if needed\n",
    "torch.save(eval_net.state_dict(), \"dqn_as.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate testing NN and load dictionary of parameters through transfer learning\n",
    "test_net= Net().to(device)\n",
    "test_net.load_state_dict(torch.load('dqn_as.dat'))\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with Unity\n",
    "n = True\n",
    "counter = 0\n",
    "while(n):\n",
    "    try:\n",
    "        env = UnityEnvironment(file_name=None, base_port=5004)\n",
    "        env.reset()\n",
    "        group_name = env.get_agent_groups()[0] \n",
    "        print('Connected...')\n",
    "        n = False\n",
    "    except:\n",
    "        counter += 1\n",
    "        time.sleep(1)\n",
    "        if (counter > 11):\n",
    "            print('Connection failed...')\n",
    "            n = False\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RewardLst1 = []  # Accumulated reward\n",
    "StepLst1 = []    # Steps per episode\n",
    "print(\"Episode:\", end = \"\")\n",
    "time.sleep(1)\n",
    "for episodeIdx in range(15):\n",
    "    env.reset()\n",
    "    time.sleep(0.5)\n",
    "    print(episodeIdx, end = \", \")\n",
    "    \n",
    "    # Get initial state of agent\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    s = torch.FloatTensor(step_result.obs[0]).to(device)\n",
    "    \n",
    "    EpochDone = False\n",
    "    rewardSum = 0\n",
    "    lossSum = 0\n",
    "    steps = 1\n",
    "    while (not EpochDone):\n",
    "        action = test_net(s)\n",
    "        MaxIdxOfEachAgent = torch.unsqueeze(torch.max(action, 1)[1], 1)\n",
    "        ActionIdxArray = MaxIdxOfEachAgent.cpu().data.numpy()\n",
    "        env.set_actions(group_name, ActionIdxArray)\n",
    "        env.step()\n",
    "        step_result = env.get_step_result(group_name)\n",
    "        s_ = step_result.obs[0]\n",
    "        s_ = torch.FloatTensor(s_).to(device)\n",
    "        reward = step_result.reward\n",
    "        rewardSum = rewardSum + np.average(reward)\n",
    "        done = step_result.done\n",
    "        for elt in done:\n",
    "            if elt:\n",
    "                EpochDone = True\n",
    "        s = s_\n",
    "        steps += 1\n",
    "    # Record data\n",
    "    RewardLst1.append(rewardSum/steps)\n",
    "    StepLst1.append(steps)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward and steps per episode\n",
    "fig = plt.figure(figsize=(18,18)) \n",
    "plt.rc('font', size=20)\n",
    "plt.rc('axes', titlesize=20)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(RewardLst1)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(StepLst1)\n",
    "plt.grid(True)\n",
    "plt.ylabel(\"Steps\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of NN\n",
    "summary(test_net, input_size=(3, 50, 25, 20, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
